{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kommasetzung\n",
    "\n",
    "Und los gehts!!! Motivationslevel :1000%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Till/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "sno = nltk.stem.SnowballStemmer('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dieses „Hardware-Anfängerbuch\" vermittelt grundlegende Hardware- \n",
      "Kenntnisse, die jeder PC-Benutzer\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "with open('untitled.txt', \"r\", encoding=\"utf-8\") as text:\n",
    "    text = text.read()\n",
    "    \n",
    "### Später entfernen!!!\n",
    "\n",
    "text = text[:]\n",
    "###\n",
    "#print(text)\n",
    "\n",
    "text = re.sub(r'\\n[0-9]+ \\n', '', text)\n",
    "text = re.sub(r'\\n\\n[a-zA-Z] \\n\\n', '', text)\n",
    "#re.sub(r'\\n\\n\\n[a-zA-Z] \\n\\n\\n', \"\", text)\n",
    "#text = text.replace(\"/n\"+isdigit()+\"/n\",\"\")\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dieses', '„Hardware-Anfängerbuch', \"''\", 'vermittelt', 'grundlegende', 'Hardware-', 'Kenntnisse', ',', 'die', 'jeder', 'PC-Benutzer', 'haben', 'sollte', '.', 'Das', 'Buch', 'ist', 'aus', 'Lehrgängen', 'für', 'Verkäuferinnen', ',', 'Apothekerinnen', ',', 'Anwälte', 'und', 'Steuerberater', 'entstanden', '.', 'Es', 'soll', 'für', 'jeden', 'verständlich', 'sein', 'und', 'bleiben', '.', 'Es', 'werden', 'nur', 'minimale', 'technische', 'Kenntnisse', 'vorausgesetzt', '.', 'Das', 'Buch', 'ist', 'für', 'Leute', 'geschrieben', ',', 'die', 'ihren', 'Computer', 'regelmäßig', 'benutzen', 'und', 'etwas', 'mehr', 'über', 'die', 'Hardware', 'wissen', 'wollen', '.', 'Es', 'ist', 'kein', 'Einsteigerbuch', 'für', 'Leute', ',', 'die', 'erstmals', 'an', 'einem', 'PC', 'sitzen', '.', 'Wie', 'man', 'Windows', '(', 'oder', 'ein', 'anderes', 'Betriebssystem', ')', 'startet', ',', 'benutzt', 'und', 'beendet', ',', 'wie', 'man', 'die', 'Maus']\n"
     ]
    }
   ],
   "source": [
    "#TOKENIZING INTO WORDS\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "woerter = word_tokenize(text)\n",
    "print(woerter[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318170\n",
      "29.846867749419953 Prozent an Wörter gespart durch Stemming\n"
     ]
    }
   ],
   "source": [
    "#STEMMING\n",
    "print(len(woerter))\n",
    "x = (len(set(woerter)))\n",
    "woerter_stem = [sno.stem(wort) for wort in woerter]\n",
    "y = (len(set(woerter_stem)))\n",
    "print(str((x-y)*100/(x)) +\" Prozent an Wörter gespart durch Stemming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 21596), ('.', 17130), ('und', 7414), ('ein', 7309), ('die', 7166), ('der', 4662), ('sie', 4569), ('ich', 4467), ('zu', 3334), ('das', 3304), ('in', 3115), ('nicht', 2854), ('den', 2823), ('war', 2750), ('es', 2623), ('er', 2604), ('mit', 2457), ('auf', 2415), ('»', 2414), ('«', 2412), (\"''\", 2335), ('ist', 2116), ('sich', 1919), ('von', 1876), ('hatt', 1873), ('``', 1822), ('?', 1620), ('dass', 1532), ('als', 1442), ('ihr', 1439), ('wie', 1396), ('an', 1359), ('sein', 1336), ('du', 1335), ('-', 1298), ('dem', 1273), ('fur', 1244), ('wenn', 1233), ('harry', 1213), ('sagt', 1203), ('wir', 1186), ('mein', 1181), ('im', 1133), ('dies', 1096), ('hab', 1092), ('was', 1054), (':', 1054), ('noch', 1053), ('um', 1044), ('aber', 1033)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(woerter_stem)\n",
    "print(counts.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GERMAN WORD CLASSES / LEXICAL CATEGORIES\n",
    "from nltk.tag import UnigramTagger\n",
    "\n",
    "'''\n",
    "    WORDS = 'words'   #: column type for words\n",
    "    POS = 'pos'       #: column type for part-of-speech tags\n",
    "    TREE = 'tree'     #: column type for parse trees\n",
    "    CHUNK = 'chunk'   #: column type for chunk structures\n",
    "    NE = 'ne'         #: column type for named entities\n",
    "    SRL = 'srl'       #: column type for semantic role labels\n",
    "    IGNORE = 'ignore' #: column type for column that should be ignored\n",
    "\n",
    "    #: A list of all column types supported by the conll corpus reader.\n",
    "    COLUMN_TYPES = (WORDS, POS, TREE, CHUNK, NE, SRL, IGNORE)\n",
    "'''\n",
    "\n",
    "root = '/Users/Till/Dropbox/Deep Learning Udacity/deep-learning/Kommasetzung'\n",
    "fileid = 'tiger_release_aug07.corrected.16012013.conll09'\n",
    "columntypes = ['ignore', 'words', 'ignore', 'ignore', 'pos']\n",
    "\n",
    "corp = nltk.corpus.ConllCorpusReader(root, fileid, columntypes, encoding='utf8')\n",
    "tagged_sents = corp.tagged_sents()\n",
    "tagger = UnigramTagger(corp.tagged_sents())\n",
    "\n",
    "#WORD CLASSIFICATION\n",
    "w_classes = tagger.tag(woerter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Dieses', 'PDAT'), ('„Hardware-Anfängerbuch', None), (\"''\", '$('), ('vermittelt', 'VVPP'), ('grundlegende', 'ADJA'), ('Hardware-', None), ('Kenntnisse', 'NN'), (',', '$,'), ('die', 'ART'), ('jeder', 'PIAT'), ('PC-Benutzer', None), ('haben', 'VAFIN'), ('sollte', 'VMFIN'), ('.', '$.'), ('Das', 'ART'), ('Buch', 'NN'), ('ist', 'VAFIN'), ('aus', 'APPR'), ('Lehrgängen', None), ('für', 'APPR'), ('Verkäuferinnen', None), (',', '$,'), ('Apothekerinnen', None), (',', '$,'), ('Anwälte', 'NN'), ('und', 'KON'), ('Steuerberater', 'NN'), ('entstanden', 'VVPP'), ('.', '$.'), ('Es', 'PPER'), ('soll', 'VMFIN'), ('für', 'APPR'), ('jeden', 'PIAT'), ('verständlich', 'ADJD'), ('sein', 'VAINF'), ('und', 'KON'), ('bleiben', 'VVINF'), ('.', '$.'), ('Es', 'PPER'), ('werden', 'VAINF'), ('nur', 'ADV'), ('minimale', 'ADJA'), ('technische', 'ADJA'), ('Kenntnisse', 'NN'), ('vorausgesetzt', 'VVPP'), ('.', '$.'), ('Das', 'ART'), ('Buch', 'NN'), ('ist', 'VAFIN'), ('für', 'APPR')]\n"
     ]
    }
   ],
   "source": [
    "print(w_classes[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "woerter_classes = []\n",
    "\n",
    "for word in w_classes:\n",
    "    if \"$\" in str(word[1]):\n",
    "        word_class = word[0] + \"c\"\n",
    "    else:\n",
    "        word_class = word[1]\n",
    "    woerter_classes.append(word_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PDAT', None, \"''c\", 'VVPP', 'ADJA', None, 'NN', ',c', 'ART', 'PIAT', None, 'VAFIN', 'VMFIN', '.c', 'ART', 'NN', 'VAFIN', 'APPR', None, 'APPR', None, ',c', None, ',c', 'NN', 'KON', 'NN', 'VVPP', '.c', 'PPER', 'VMFIN', 'APPR', 'PIAT', 'ADJD', 'VAINF', 'KON', 'VVINF', '.c', 'PPER', 'VAINF', 'ADV', 'ADJA', 'ADJA', 'NN', 'VVPP', '.c', 'ART', 'NN', 'VAFIN', 'APPR']\n"
     ]
    }
   ],
   "source": [
    "print(woerter_classes[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22740\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "\n",
    "counts_classes = Counter(woerter_classes)\n",
    "vocab_classes = sorted(counts_classes, key=counts_classes.get, reverse=True)\n",
    "\n",
    "vocabs = vocab + vocab_classes\n",
    "print(len(vocabs))\n",
    "\n",
    "vocabs_to_int = {wort: i for i, wort in enumerate(vocabs, 1)}\n",
    "int_to_vocabs = {wort: i for i, wort in vocabs_to_int.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22713, 22678, 22702, 22692, 22689, 22678, 22679, 22681, 22680, 22699, 22678, 22687, 22697, 22685, 22680, 22679, 22687, 22682, 22678, 22682, 22678, 22681, 22678, 22681, 22679, 22688, 22679, 22692, 22685, 22683, 22697, 22682, 22699, 22690, 22708, 22688, 22691, 22685, 22683, 22708, 22684, 22689, 22689, 22679, 22692, 22685, 22680, 22679, 22687, 22682, 22679, 22692, 22681, 22680, 22694, 22679, 22690, 22691, 22688, 22703, 22684, 22682, 22680, 22679, 22691, 22697, 22685, 22683, 22687, 22699, 22678, 22682, 22679, 22681, 22680, 22684, 22682, 22680, 22679, 22686, 22685, 22716, 22703, 22693, 22718, 22688, 22680, 22703, 22679, 22717, 22686, 22681, 22692, 22688, 22692, 22681, 22712, 22703, 22680, 22679]\n"
     ]
    }
   ],
   "source": [
    "woerter_ints = [vocabs_to_int[wort] for wort in woerter_stem]\n",
    "woerter_classes_ints = [vocabs_to_int[wort] for wort in woerter_classes]\n",
    "print(woerter_classes_ints[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling (VLLT später mal)\n",
    "\n",
    "Words that show up often such as \"the\", \"of\", and \"for\" don't provide much context to the nearby words. If we discard some of them, we can remove some of the noise from our data and in return get faster training and better representations. This process is called subsampling by Mikolov. For each word $w_i$ in the training set, we'll discard it with probability given by \n",
    "\n",
    "$$ P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}} $$\n",
    "\n",
    "where $t$ is a threshold parameter and $f(w_i)$ is the frequency of word $w_i$ in the total dataset.\n",
    "\n",
    "I'm going to leave this up to you as an exercise. Check out my solution to see how I did it.\n",
    "\n",
    "> **Exercise:** Implement subsampling for the words in `int_words`. That is, go through `int_words` and discard each word given the probablility $P(w_i)$ shown above. Note that $P(w_i)$ is that probability that a word is discarded. Assign the subsampled data to `train_words`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "from time import time\n",
    "t0 = time()\n",
    "\n",
    "\n",
    "threshold = 1e-5\n",
    "word_counts = Counter(int_words)\n",
    "total_count = len(int_words)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
    "train_words = [word for word in int_words if random.random() < (1 - p_drop[word])]\n",
    "\n",
    "print(\"processing time:\", round(time()-t0,3), \"s\")\n",
    "\n",
    "print(len(int_words))\n",
    "print(len(train_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining labels\n",
    "\n",
    "Wenn Wort \",\" ist dann Label 1 sonst 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      "['Dieses', '„Hardware-Anfängerbuch', \"''\", 'vermittelt', 'grundlegende', 'Hardware-', 'Kenntnisse', ',', 'die', 'jeder', 'PC-Benutzer', 'haben', 'sollte', '.', 'Das']\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "#for idx, wort in enumerate(woerter,0):\n",
    "for wort in woerter:\n",
    "    if wort == \",\":\n",
    "        labels.append(1)\n",
    "        #labels.append([1, wort])\n",
    "    else:\n",
    "        labels.append(0)\n",
    "        #labels.append([0, wort])    \n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print(labels[:15])\n",
    "print(woerter[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ich will eine Anzahl an Wörtern um ein Leerzeichen herum haben. Sie werden als Batch eingefüttert und heraus soll eine eine 0 oder 1 kommen (Prediction) ob an dieser Leerzeichenstelle ein Komma hinkommt oder nicht\n",
    "\n",
    "From [Mikolov et al.](https://arxiv.org/pdf/1301.3781.pdf): \n",
    "\n",
    "\"Since the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from those words in our training examples... If we choose $C = 5$, for each training word we will select randomly a number $R$ in range $< 1; C >$, and then use $R$ words from history and $R$ words from the future of the current word as correct labels.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(woerter_ints, idx, window_size=10): # woerter_ints should be woerter_ints or woerter_classes_ints \n",
    "                                  \n",
    "    r = np.random.randint(1, window_size+1)\n",
    "    \n",
    "    #untere Boundaries\n",
    "    if idx - r < 0:\n",
    "        minus_idx = 0\n",
    "    else:\n",
    "        minus_idx = idx - r\n",
    "        \n",
    "    #obere Boundaries    \n",
    "    if idx + r > len(woerter_ints):\n",
    "        plus_idx = len(woerter_ints)\n",
    "    else:\n",
    "        plus_idx = idx + r\n",
    "        \n",
    "        \n",
    "    #IF WORT IS A KOMMA:\n",
    "    \n",
    "    if int_to_vocabs[woerter_ints[idx]] == \",\":\n",
    "        davor = woerter_ints[minus_idx:idx]\n",
    "        danach = woerter_ints[idx+1:plus_idx+1]\n",
    "        \n",
    "    \n",
    "    #IF WORT IS NOT A KOMMA:\n",
    "    else:\n",
    "        davor = woerter_ints[minus_idx:idx]\n",
    "        danach = woerter_ints[idx:plus_idx]\n",
    "        \n",
    "    davor = list(filter(lambda a: a != vocabs_to_int[\",\"], davor))\n",
    "    davor = list(filter(lambda a: a != vocabs_to_int[\",c\"], davor))\n",
    "    \n",
    "    danach = list(filter(lambda a: a != vocabs_to_int[\",\"], danach))\n",
    "    danach = list(filter(lambda a: a != vocabs_to_int[\",c\"], danach))\n",
    "    \n",
    "    if len(danach) > len(davor):\n",
    "        danach = danach[:-1]\n",
    "    elif len(danach) < len(davor):\n",
    "        davor = davor[1:]\n",
    "    \n",
    "    x_davor = ([0]*(window_size-len(davor))) + davor\n",
    "    x_danach = danach + ([0]*(window_size-len(danach)))\n",
    "    \n",
    "    x = x_davor + x_danach\n",
    "\n",
    "    return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(318170, 40)\n",
      "[[    0     0     0     0     0     0     0     0  9933    45   114     2\n",
      "      0     0     0     0     0     0     0     0     0     0 22692 22689\n",
      "  22678 22679 22680 22699 22678 22687 22697 22685 22680 22679 22687 22682\n",
      "  22678 22682     0     0]\n",
      " [    0     0     0     0  3205     5    83  9933    45   114     2    10\n",
      "    273    22    53  9934     0     0     0     0     0     0     0     0\n",
      "      0 22680 22699 22678 22687 22697 22685 22680 22679 22687 22682     0\n",
      "      0     0     0     0]\n",
      " [    0     0     0     0     5    83  9933    45   114     2    10   273\n",
      "     22    53  9934    37     0     0     0     0     0     0     0     0\n",
      "      0 22699 22678 22687 22697 22685 22680 22679 22687 22682 22678     0\n",
      "      0     0     0     0]]\n",
      "[0 0 0]\n",
      "['sollte', '.', 'Das']\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "feature_woerter = [get_batch(woerter_ints, idx, window_size=10) for idx in range(0,len(woerter_ints))]\n",
    "feature_classes =  [get_batch(woerter_classes_ints, idx, window_size=10) for idx in range(0,len(woerter_ints))]\n",
    "features = list(zip(feature_woerter, feature_classes))\n",
    "features = np.asarray(features).reshape(len(woerter_ints), -1)\n",
    "\n",
    "print(features.shape)\n",
    "\n",
    "print(features[12:15])  \n",
    "print(labels[12:15])\n",
    "print(woerter[12:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "navigationsgerat\n",
      "[9933, 45, 114, 2, 10, 273, 22, 53, 9934, 37]\n",
      "[22678, 22687, 22697, 22685, 22680, 22679, 22687, 22682, 22678, 22682]\n"
     ]
    }
   ],
   "source": [
    "print(int_to_vocabs[11295])\n",
    "print(woerter_ints[10:20])\n",
    "print(woerter_classes_ints[10:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(318170, 40)\n",
      "(318170,)\n",
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(254536, 40) \n",
      "Validation set: \t(31817, 40) \n",
      "Test set: \t\t(31817, 40) \n",
      "Woerter set: \t\t(31817, 40)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "split_idx = int(len(features)*0.8)\n",
    "\n",
    "print(features.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "train_x, val_x = features[:split_idx], features[split_idx:]\n",
    "train_y, val_y = labels[:split_idx], labels[split_idx:]\n",
    "train_woerter, val_woerter = woerter_stem[:split_idx], woerter_stem[split_idx:]\n",
    "\n",
    "test_idx = int(len(val_x)*0.5)\n",
    "val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "val_y, test_y = val_y[:test_idx], val_y[test_idx:]\n",
    "val_woerter, test_woerter = val_woerter[:test_idx], val_woerter[test_idx:]\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape),\n",
    "    \"\\nWoerter set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the graph\n",
    "Here, we'll build the graph. First up, defining the hyperparameters.\n",
    "\n",
    "lstm_size: Number of units in the hidden layers in the LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512, etc.\n",
    "lstm_layers: Number of LSTM layers in the network. I'd start with 1, then add more if I'm underfitting.\n",
    "batch_size: The number of reviews to feed the network in one training pass. Typically this should be set as high as you can go without running out of memory.\n",
    "learning_rate: Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_size = 128\n",
    "lstm_layers = 3\n",
    "batch_size = 100\n",
    "learning_rate = 0.0005 #0.0005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the network itself, we'll be passing in our 200 element long review vectors. Each batch will be `batch_size` vectors. We'll also be using dropout on the LSTM layer, so we'll make a placeholder for the keep probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_words = (len(vocabs_to_int)) + 1 # Adding 1 because we use 0's for padding, dictionary started at 1\n",
    "\n",
    "\n",
    "# Create the graph object\n",
    "graph = tf.Graph()\n",
    "# Add nodes to the graph\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "Now we'll add an embedding layer. We need to do this because there are 74000 words in our vocabulary. It is massively inefficient to one-hot encode our classes here. You should remember dealing with this problem from the word2vec lesson. Instead of one-hot encoding, we can have an embedding layer and use that layer as a lookup table. You could train an embedding layer using word2vec, then load it here. But, it's fine to just make a new layer and let the network learn the weights.\n",
    "\n",
    "> **Exercise:** Create the embedding lookup matrix as a `tf.Variable`. Use that embedding matrix to get the embedded vectors to pass to the LSTM cell with [`tf.nn.embedding_lookup`](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup). This function takes the embedding matrix and an input tensor, such as the review vectors. Then, it'll return another tensor with the embedded vectors. So, if the embedding layer as 200 units, the function will return a tensor with size [batch_size, 200].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Size of the embedding vectors (number of units in the embedding layer)\n",
    "embed_size = 300 \n",
    "\n",
    "with graph.as_default():\n",
    "    embedding = tf.Variable(tf.truncated_normal((n_words, embed_size), stddev = 0.3))\n",
    "    #embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # Your basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "    \n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output\n",
    "\n",
    "We only care about the final output, we'll be using that as our sentiment prediction. So we need to grab the last output with `outputs[:, -1]`, the calculate the cost from that and `labels_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)\n",
    "    cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation accuracy\n",
    "\n",
    "Here we can add a few nodes to calculate the accuracy which we'll use in the validation pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching\n",
    "\n",
    "This is a simple function for returning batches from our data. First it removes data such that we only have full batches. Then it iterates through the `x` and `y` arrays and returns slices out of those arrays with size `[batch_size]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, woerter, batch_size=100):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y, woerter = x[:n_batches*batch_size], y[:n_batches*batch_size], woerter[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size], woerter[ii:ii+batch_size]### Batching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Below is the typical training code. If you want to do this yourself, feel free to delete all this code and implement it yourself. Before you run this, make sure the `checkpoints` directory exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/3 Iteration: 100 Train loss: 0.249\n",
      "Val acc: 0.925\n",
      "Epoch: 0/3 Iteration: 110 Train loss: 0.040\n",
      "Epoch: 0/3 Iteration: 120 Train loss: 0.049\n",
      "Epoch: 0/3 Iteration: 130 Train loss: 0.049\n",
      "Epoch: 0/3 Iteration: 140 Train loss: 0.021\n",
      "Epoch: 0/3 Iteration: 150 Train loss: 0.029\n",
      "Val acc: 0.925\n",
      "Epoch: 0/3 Iteration: 160 Train loss: 0.189\n",
      "Epoch: 0/3 Iteration: 170 Train loss: 0.091\n",
      "Epoch: 0/3 Iteration: 180 Train loss: 0.084\n",
      "Epoch: 0/3 Iteration: 190 Train loss: 0.059\n",
      "Epoch: 0/3 Iteration: 200 Train loss: 0.057\n",
      "Val acc: 0.925\n",
      "Epoch: 0/3 Iteration: 210 Train loss: 0.068\n",
      "Epoch: 0/3 Iteration: 220 Train loss: 0.047\n",
      "Epoch: 0/3 Iteration: 230 Train loss: 0.029\n",
      "Epoch: 0/3 Iteration: 240 Train loss: 0.011\n",
      "Epoch: 0/3 Iteration: 250 Train loss: 0.039\n",
      "Val acc: 0.925\n",
      "Epoch: 0/3 Iteration: 260 Train loss: 0.057\n",
      "Epoch: 0/3 Iteration: 270 Train loss: 0.040\n",
      "Epoch: 0/3 Iteration: 280 Train loss: 0.058\n",
      "Epoch: 0/3 Iteration: 290 Train loss: 0.039\n",
      "Epoch: 0/3 Iteration: 300 Train loss: 0.002\n",
      "Val acc: 0.925\n",
      "Epoch: 0/3 Iteration: 310 Train loss: 0.037\n",
      "Epoch: 0/3 Iteration: 320 Train loss: 0.030\n",
      "Epoch: 0/3 Iteration: 330 Train loss: 0.066\n",
      "Epoch: 0/3 Iteration: 340 Train loss: 0.039\n",
      "Epoch: 0/3 Iteration: 350 Train loss: 0.057\n",
      "Val acc: 0.925\n",
      "Epoch: 0/3 Iteration: 360 Train loss: 0.048\n",
      "Epoch: 0/3 Iteration: 370 Train loss: 0.056\n",
      "Epoch: 0/3 Iteration: 380 Train loss: 0.028\n",
      "Epoch: 0/3 Iteration: 390 Train loss: 0.058\n",
      "Epoch: 0/3 Iteration: 400 Train loss: 0.039\n",
      "Val acc: 0.925\n",
      "Epoch: 0/3 Iteration: 410 Train loss: 0.067\n",
      "Epoch: 0/3 Iteration: 420 Train loss: 0.057\n",
      "Epoch: 0/3 Iteration: 430 Train loss: 0.067\n",
      "Epoch: 0/3 Iteration: 440 Train loss: 0.029\n",
      "Epoch: 0/3 Iteration: 450 Train loss: 0.038\n",
      "Val acc: 0.925\n",
      "Epoch: 0/3 Iteration: 460 Train loss: 0.058\n",
      "Epoch: 0/3 Iteration: 470 Train loss: 0.030\n",
      "Epoch: 0/3 Iteration: 480 Train loss: 0.057\n",
      "Epoch: 0/3 Iteration: 490 Train loss: 0.039\n",
      "Epoch: 0/3 Iteration: 500 Train loss: 0.040\n",
      "Val acc: 0.925\n",
      "Epoch: 0/3 Iteration: 510 Train loss: 0.039\n",
      "Epoch: 0/3 Iteration: 520 Train loss: 0.048\n",
      "Epoch: 0/3 Iteration: 530 Train loss: 0.029\n",
      "Epoch: 0/3 Iteration: 540 Train loss: 0.037\n",
      "Epoch: 0/3 Iteration: 550 Train loss: 0.048\n",
      "Val acc: 0.925\n",
      "Epoch: 0/3 Iteration: 560 Train loss: 0.039\n",
      "Epoch: 0/3 Iteration: 570 Train loss: 0.058\n",
      "Epoch: 0/3 Iteration: 580 Train loss: 0.039\n",
      "Epoch: 0/3 Iteration: 590 Train loss: 0.039\n",
      "Epoch: 0/3 Iteration: 600 Train loss: 0.039\n",
      "Val acc: 0.925\n",
      "Epoch: 0/3 Iteration: 610 Train loss: 0.039\n",
      "Epoch: 0/3 Iteration: 620 Train loss: 0.039\n",
      "Epoch: 0/3 Iteration: 630 Train loss: 0.039\n",
      "Epoch: 0/3 Iteration: 640 Train loss: 0.056\n",
      "Epoch: 0/3 Iteration: 650 Train loss: 0.047\n",
      "Val acc: 0.925\n",
      "Epoch: 0/3 Iteration: 660 Train loss: 0.029\n",
      "Epoch: 0/3 Iteration: 670 Train loss: 0.083\n",
      "Epoch: 0/3 Iteration: 680 Train loss: 0.056\n",
      "Epoch: 0/3 Iteration: 690 Train loss: 0.049\n",
      "Epoch: 0/3 Iteration: 700 Train loss: 0.066\n",
      "Val acc: 0.925\n",
      "Epoch: 0/3 Iteration: 710 Train loss: 0.030\n",
      "Epoch: 0/3 Iteration: 720 Train loss: 0.019\n",
      "Epoch: 0/3 Iteration: 730 Train loss: 0.038\n",
      "Epoch: 0/3 Iteration: 740 Train loss: 0.048\n",
      "Epoch: 0/3 Iteration: 750 Train loss: 0.039\n",
      "Val acc: 0.925\n",
      "Epoch: 0/3 Iteration: 760 Train loss: 0.048\n",
      "Epoch: 0/3 Iteration: 770 Train loss: 0.039\n",
      "Epoch: 0/3 Iteration: 780 Train loss: 0.019\n",
      "Epoch: 0/3 Iteration: 790 Train loss: 0.047\n",
      "Epoch: 0/3 Iteration: 800 Train loss: 0.059\n",
      "Val acc: 0.925\n",
      "Epoch: 0/3 Iteration: 810 Train loss: 0.064\n",
      "Epoch: 0/3 Iteration: 820 Train loss: 0.038\n",
      "Epoch: 0/3 Iteration: 830 Train loss: 0.063\n",
      "Epoch: 0/3 Iteration: 840 Train loss: 0.029\n",
      "Epoch: 0/3 Iteration: 850 Train loss: 0.035\n",
      "Val acc: 0.925\n",
      "Epoch: 0/3 Iteration: 860 Train loss: 0.042\n",
      "Epoch: 0/3 Iteration: 870 Train loss: 0.038\n",
      "Epoch: 0/3 Iteration: 880 Train loss: 0.051\n",
      "Epoch: 0/3 Iteration: 890 Train loss: 0.087\n",
      "Epoch: 0/3 Iteration: 900 Train loss: 0.007\n",
      "Val acc: 0.925\n",
      "Epoch: 0/3 Iteration: 910 Train loss: 0.044\n",
      "Epoch: 0/3 Iteration: 920 Train loss: 0.066\n",
      "Epoch: 0/3 Iteration: 930 Train loss: 0.049\n",
      "Epoch: 0/3 Iteration: 940 Train loss: 0.038\n",
      "Epoch: 0/3 Iteration: 950 Train loss: 0.065\n",
      "Val acc: 0.925\n",
      "Epoch: 0/3 Iteration: 960 Train loss: 0.095\n",
      "Epoch: 0/3 Iteration: 970 Train loss: 0.058\n",
      "Epoch: 0/3 Iteration: 980 Train loss: 0.059\n",
      "Epoch: 0/3 Iteration: 990 Train loss: 0.081\n",
      "Epoch: 0/3 Iteration: 1000 Train loss: 0.062\n",
      "Val acc: 0.925\n",
      "Epoch: 0/3 Iteration: 1010 Train loss: 0.041\n",
      "Epoch: 0/3 Iteration: 1020 Train loss: 0.068\n",
      "Epoch: 0/3 Iteration: 1030 Train loss: 0.080\n",
      "Epoch: 0/3 Iteration: 1040 Train loss: 0.039\n",
      "Epoch: 0/3 Iteration: 1050 Train loss: 0.043\n",
      "Val acc: 0.925\n",
      "Epoch: 0/3 Iteration: 1060 Train loss: 0.060\n",
      "Epoch: 0/3 Iteration: 1070 Train loss: 0.076\n",
      "Epoch: 0/3 Iteration: 1080 Train loss: 0.066\n",
      "Epoch: 0/3 Iteration: 1090 Train loss: 0.057\n",
      "Epoch: 0/3 Iteration: 1100 Train loss: 0.059\n",
      "Val acc: 0.926\n",
      "Epoch: 0/3 Iteration: 1110 Train loss: 0.040\n",
      "Epoch: 0/3 Iteration: 1120 Train loss: 0.091\n",
      "Epoch: 0/3 Iteration: 1130 Train loss: 0.067\n",
      "Epoch: 0/3 Iteration: 1140 Train loss: 0.066\n",
      "Epoch: 0/3 Iteration: 1150 Train loss: 0.076\n",
      "Val acc: 0.925\n",
      "Epoch: 0/3 Iteration: 1160 Train loss: 0.035\n",
      "Epoch: 0/3 Iteration: 1170 Train loss: 0.073\n",
      "Epoch: 0/3 Iteration: 1180 Train loss: 0.103\n",
      "Epoch: 0/3 Iteration: 1190 Train loss: 0.064\n",
      "Epoch: 0/3 Iteration: 1200 Train loss: 0.116\n",
      "Val acc: 0.926\n",
      "Epoch: 0/3 Iteration: 1210 Train loss: 0.068\n",
      "Epoch: 0/3 Iteration: 1220 Train loss: 0.100\n",
      "Epoch: 0/3 Iteration: 1230 Train loss: 0.058\n",
      "Epoch: 0/3 Iteration: 1240 Train loss: 0.074\n",
      "Epoch: 0/3 Iteration: 1250 Train loss: 0.047\n",
      "Val acc: 0.926\n",
      "Epoch: 0/3 Iteration: 1260 Train loss: 0.056\n",
      "Epoch: 0/3 Iteration: 1270 Train loss: 0.084\n",
      "Epoch: 0/3 Iteration: 1280 Train loss: 0.056\n",
      "Epoch: 0/3 Iteration: 1290 Train loss: 0.053\n",
      "Epoch: 0/3 Iteration: 1300 Train loss: 0.066\n",
      "Val acc: 0.925\n",
      "Epoch: 0/3 Iteration: 1310 Train loss: 0.032\n",
      "Epoch: 0/3 Iteration: 1320 Train loss: 0.062\n",
      "Epoch: 0/3 Iteration: 1330 Train loss: 0.090\n",
      "Epoch: 0/3 Iteration: 1340 Train loss: 0.077\n",
      "Epoch: 0/3 Iteration: 1350 Train loss: 0.053\n",
      "Val acc: 0.925\n",
      "Epoch: 0/3 Iteration: 1360 Train loss: 0.043\n",
      "Epoch: 0/3 Iteration: 1370 Train loss: 0.073\n",
      "Epoch: 0/3 Iteration: 1380 Train loss: 0.083\n",
      "Epoch: 0/3 Iteration: 1390 Train loss: 0.048\n",
      "Epoch: 0/3 Iteration: 1400 Train loss: 0.039\n",
      "Val acc: 0.925\n",
      "Epoch: 0/3 Iteration: 1410 Train loss: 0.059\n",
      "Epoch: 0/3 Iteration: 1420 Train loss: 0.077\n",
      "Epoch: 0/3 Iteration: 1430 Train loss: 0.046\n",
      "Epoch: 0/3 Iteration: 1440 Train loss: 0.033\n",
      "Epoch: 0/3 Iteration: 1450 Train loss: 0.070\n",
      "Val acc: 0.925\n",
      "Epoch: 0/3 Iteration: 1460 Train loss: 0.066\n",
      "Epoch: 0/3 Iteration: 1470 Train loss: 0.089\n",
      "Epoch: 0/3 Iteration: 1480 Train loss: 0.086\n",
      "Epoch: 0/3 Iteration: 1490 Train loss: 0.057\n",
      "Epoch: 0/3 Iteration: 1500 Train loss: 0.074\n",
      "Val acc: 0.925\n",
      "Epoch: 0/3 Iteration: 1510 Train loss: 0.038\n",
      "Epoch: 0/3 Iteration: 1520 Train loss: 0.054\n",
      "Epoch: 0/3 Iteration: 1530 Train loss: 0.094\n",
      "Epoch: 0/3 Iteration: 1540 Train loss: 0.041\n",
      "Epoch: 0/3 Iteration: 1550 Train loss: 0.068\n",
      "Val acc: 0.925\n",
      "Epoch: 0/3 Iteration: 1560 Train loss: 0.045\n",
      "Epoch: 0/3 Iteration: 1570 Train loss: 0.072\n",
      "Epoch: 0/3 Iteration: 1580 Train loss: 0.074\n",
      "Epoch: 0/3 Iteration: 1590 Train loss: 0.048\n",
      "Epoch: 0/3 Iteration: 1600 Train loss: 0.040\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-5cbc00564942>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m                             \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                             initial_state: val_state}\n\u001b[0;32m---> 30\u001b[0;31m                     \u001b[0mbatch_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                     \u001b[0mval_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Val acc: {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 100\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        # How Lables are exppected: [[0][1][0]\n",
    "        #How it could work: y = [[int(label[0])] for label in y] and labels_: y\n",
    "        \n",
    "        for ii, (x, y, wort) in enumerate(get_batches(train_x, train_y, train_woerter, batch_size), 1):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y[:,None],\n",
    "                    keep_prob: 0.5,\n",
    "                    initial_state: state}\n",
    "            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n",
    "            \n",
    "            if iteration%10==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "\n",
    "            if iteration%50==0:\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for x, y, wort in get_batches(val_x, val_y, val_woerter, batch_size):\n",
    "                    feed = {inputs_: x,\n",
    "                            labels_: y[:,None],\n",
    "                            keep_prob: 1,\n",
    "                            initial_state: val_state}\n",
    "                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "            iteration +=1\n",
    "    saver.save(sess, \"checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.922\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_acc = []\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for ii, (x, y, wort) in enumerate(get_batches(test_x, test_y, test_woerter, batch_size), 1):\n",
    "        feed = {inputs_: x,\n",
    "                labels_: y[:, None],\n",
    "                keep_prob: 1,\n",
    "                initial_state: test_state}\n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.3f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_list = []\n",
    "batch_size = 100\n",
    "counter_max = 3\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    for ii, (x, y, wort) in enumerate(get_batches(test_x, test_y, test_woerter, batch_size), 1):\n",
    "        feed = {inputs_: x,\n",
    "                labels_: y[:, None],\n",
    "                keep_prob: 1,\n",
    "                initial_state: val_state}\n",
    "        prediction = sess.run(predictions, feed_dict=feed)\n",
    "    \n",
    "        for i in range(0,len(prediction)):\n",
    "            predictions_list.append([round(float(prediction[i]),4), y[i], wort[i]])\n",
    "        \n",
    "        ###    \n",
    "        if counter == counter_max:\n",
    "            break\n",
    "        counter += 1\n",
    "        ###\n",
    "        \n",
    "        \n",
    "        \n",
    "prediction_list = list(predictions_list)\n",
    "for iii in range(0, len(predictions_list)):\n",
    "    print(predictions_list[iii])\n",
    "#print(predictions_list)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import UnigramTagger\n",
    "\n",
    "\n",
    "def preprocess(text_name, vocabs_to_int=vocabs_to_int, int_to_vocabs=int_to_vocabs):\n",
    "    ##Text öffnen\n",
    "    with open(text_name, \"r\", encoding=\"utf-8\") as text:\n",
    "        text = text.read()\n",
    "    \n",
    "    ## Tokenizing\n",
    "    woerter = word_tokenize(text)\n",
    "    \n",
    "    ## Stemming\n",
    "    sno = nltk.stem.SnowballStemmer('german')\n",
    "    woerter_stem = [sno.stem(wort) for wort in woerter]\n",
    "    \n",
    "    ## Wort-Klassen\n",
    "    root = '/Users/Till/Dropbox/Deep Learning Udacity/deep-learning/Kommasetzung'\n",
    "    fileid = 'tiger_release_aug07.corrected.16012013.conll09'\n",
    "    columntypes = ['ignore', 'words', 'ignore', 'ignore', 'pos']\n",
    "    \n",
    "    corp = nltk.corpus.ConllCorpusReader(root, fileid, columntypes, encoding='utf8')\n",
    "    tagged_sents = corp.tagged_sents()\n",
    "    tagger = UnigramTagger(corp.tagged_sents())\n",
    "    w_classes = tagger.tag(woerter)\n",
    "    \n",
    "    woerter_classes = []\n",
    "\n",
    "    for word in w_classes:\n",
    "        if \"$\" in str(word[1]):\n",
    "            word_class = word[0] + \"c\"\n",
    "        else:\n",
    "            word_class = word[1]\n",
    "        woerter_classes.append(word_class)\n",
    "    \n",
    "    ## In Zahlen umwandeln\n",
    "    woerter_ints = [vocabs_to_int[wort] if wort in vocabs_to_int else vocabs_to_int[None] for wort in woerter_stem]\n",
    "    woerter_classes_ints = [vocabs_to_int[wort] if wort in vocabs_to_int else vocabs_to_int[None] for wort in woerter_classes]\n",
    "    \n",
    "    ## Features generieren\n",
    "    features = []\n",
    "    feature_woerter = [get_batch(woerter_ints, idx, window_size=10) for idx in range(0,len(woerter_ints))]\n",
    "    feature_classes =  [get_batch(woerter_classes_ints, idx, window_size=10) for idx in range(0,len(woerter_ints))]\n",
    "    features = list(zip(feature_woerter, feature_classes))\n",
    "    features = np.asarray(features).reshape(len(woerter_ints), -1)\n",
    "    features = np.ndarray.tolist(features)\n",
    "    \n",
    "    return features, woerter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_list = []\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    x, woerter = preprocess(\"test.txt\")\n",
    "    \n",
    "    for ii, (x, y, wort) in enumerate(get_batches(x, test_y, woerter, batch_size), 1):\n",
    "        \n",
    "        feed = {inputs_: x,\n",
    "                labels_: y[:, None],\n",
    "                keep_prob: 1,\n",
    "                initial_state: val_state}\n",
    "        prediction = sess.run(predictions, feed_dict=feed)\n",
    "    \n",
    "        for i in range(0,len(prediction)):\n",
    "            predictions_list.append([round(float(prediction[i]),4), wort[i]])\n",
    "        \n",
    "        \n",
    "prediction_list = list(predictions_list)\n",
    "for iii in range(0, len(predictions_list)):\n",
    "    print(predictions_list[iii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
